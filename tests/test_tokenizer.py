#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
test_markup
----------------------------------

Tests for `markup` module.
"""
# pylint: disable=I0011,W0621
import os
import pytest

from mindmeld.tokenizer import Tokenizer

APP_NAME = "food_ordering"
APP_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), APP_NAME)


@pytest.fixture
def tokenizer():
    return Tokenizer()


@pytest.fixture
def custom_tokenizer():
    return Tokenizer(app_path=APP_PATH)


def test_tokenize_raw(tokenizer):
    tokens = tokenizer.tokenize_raw("Test: Query for $500,000.")

    assert len(tokens)
    assert tokens[0]["text"] == "Test:"
    assert tokens[0]["start"] == 0
    assert tokens[3]["start"] == 16
    assert tokens[3]["start"] == 16


def test_tokenize(tokenizer):
    tokens = tokenizer.tokenize("Test: Query for $500,000. Chyea!")

    assert len(tokens)
    assert tokens[0]["entity"] == "test"
    assert tokens[0]["raw_entity"] == "Test:"
    assert tokens[1]["raw_start"] == 6
    assert tokens[3]["raw_entity"] == "$500,000."
    assert tokens[3]["raw_start"] == 16
    assert tokens[3]["entity"] == "$500,000"
    assert tokens[4]["entity"] == "chyea"
    assert tokens[4]["raw_entity"] == "Chyea!"
    assert tokens[4]["raw_start"] == 26


def test_normalize(tokenizer):
    normalized_text = tokenizer.normalize("Test: Query for $500,000.", False)

    assert normalized_text == "test query for $500,000"


def test_normalize_2(tokenizer):
    normalized_text = tokenizer.normalize(
        "Test: Query for test.12.345..test,test", False
    )

    assert normalized_text == "test query for test 12.345 test test"


def test_normalize_3(tokenizer):
    normalized_text = tokenizer.normalize("Test: awesome band sigur r√≥s.", False)

    assert normalized_text == "test awesome band sigur ros"


def test_normalize_4(tokenizer):
    normalized_text = tokenizer.normalize("D'Angelo's new album", False)

    assert normalized_text == "d angelo s new album"


def test_normalize_5(tokenizer):
    raw = "is s.o.b. ,, gonna be on at 8 p.m.?"
    normalized = tokenizer.normalize(raw, False)

    assert normalized == "is s o b gonna be on at 8 p m"


def test_normalize_apos(tokenizer):

    # verify that apostrophe at the end of a possessive form is removed
    raw = "join Dennis' pmr"
    normalized = tokenizer.normalize(raw, True)

    assert normalized == "join dennis pmr"

    # verify that apostrophe in the middle of an entity is not removed
    raw = "join O'reilly's pmr"
    normalized = tokenizer.normalize(raw, True)

    assert normalized == "join o'reilly 's pmr"


def test_mapping(tokenizer):
    raw = "Test: 1. 2. 3."
    normalized = tokenizer.normalize(raw)

    assert normalized == "test 1 2 3"

    forward, backward = tokenizer.get_char_index_map(raw, normalized)

    assert forward == {
        0: 0,
        1: 1,
        2: 2,
        3: 3,
        4: 3,
        5: 4,
        6: 5,
        7: 5,
        8: 6,
        9: 7,
        10: 7,
        11: 8,
        12: 9,
        13: 9,
    }

    assert backward == {
        0: 0,
        1: 1,
        2: 2,
        3: 3,
        4: 5,
        5: 6,
        6: 8,
        7: 9,
        8: 11,
        9: 12,
    }


def test_mapping_2(tokenizer):
    raw = "is s.o.b. ,, gonna be on at 8 p.m.?"
    normalized = tokenizer.normalize(raw, False)

    assert normalized == "is s o b gonna be on at 8 p m"

    forward, backward = tokenizer.get_char_index_map(raw, normalized)

    assert forward == {
        0: 0,
        1: 1,
        2: 2,
        3: 3,
        4: 4,
        5: 5,
        6: 6,
        7: 7,
        8: 7,
        9: 8,
        10: 8,
        11: 8,
        12: 8,
        13: 9,
        14: 10,
        15: 11,
        16: 12,
        17: 13,
        18: 14,
        19: 15,
        20: 16,
        21: 17,
        22: 18,
        23: 19,
        24: 20,
        25: 21,
        26: 22,
        27: 23,
        28: 24,
        29: 25,
        30: 26,
        31: 27,
        32: 28,
        33: 28,
        34: 28,
    }

    assert backward == {
        0: 0,
        1: 1,
        2: 2,
        3: 3,
        4: 4,
        5: 5,
        6: 6,
        7: 7,
        8: 9,
        9: 13,
        10: 14,
        11: 15,
        12: 16,
        13: 17,
        14: 18,
        15: 19,
        16: 20,
        17: 21,
        18: 22,
        19: 23,
        20: 24,
        21: 25,
        22: 26,
        23: 27,
        24: 28,
        25: 29,
        26: 30,
        27: 31,
        28: 32,
    }


def test_custom_tokenizer(tokenizer, custom_tokenizer):
    """Compares behavior across custom and default tokenizers"""
    raw = "is s.o.b. ,, gonna be on at 8 p.m.?"
    normalized = tokenizer.normalize(raw, False)
    custom_normalized = custom_tokenizer.normalize(raw, True)

    assert normalized == "is s o b gonna be on at 8 p m"
    assert custom_normalized == "is s.o.b. gonna be on at 8 p.m."


def test_custom_tokenizer_extra_special_chars(tokenizer, custom_tokenizer):
    """Compares behavior across custom and default tokenizers for special characters
    that have not been allowed in config"""
    raw = "join O'reilly's pmr."
    normalized = tokenizer.normalize(raw, True)
    custom_normalized = custom_tokenizer.normalize(raw, True)

    # Default MM behavior retains apos in the middle of words but doesn't recognize
    # periods at the end.
    # The custom tokenizer config defined accepts tokens ending with period but no apos
    # as part of the tokens.
    assert normalized == "join o'reilly 's pmr"
    assert custom_normalized == 'join o reilly s pmr.'
